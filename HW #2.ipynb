{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW #2 Exercises\n",
    "\n",
    "This month, we want to start thinking about reducing the linguistic complexity of our trial text. How can we process our text and turn it into a set of terms or n-grams?\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Now that we've gone through homework # 1, we have built a data structure with an appropriate set of labels and associated meta data.  Your thinking about the best way to model the data may have evolved after today's discussion. Think about, and build, the data structure that will best accomodate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "It's a safe bet that after exercise 1, you still probably want to retain a large list of tokens from the text.\n",
    "Using NLTK or SPACY, which elements of our trial data should we tokenize? Select the elements of our data structure we wish to tokenize and write code that will convert these elements into tokens. Should we store all tokens together, or maintain logical separation? What are the drawbacks of each of these choices?  Remember that we should have the ability to easily change how we tokenize these elements (1-grams, 2-grams, noun chunks, root words, etc) but choose the approach you think is most appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Do you notice anything about your tokens? Are there a lot of seemingly useless or ambiguous tokens - what makes you think they are useless? Can you think of any ways to automate the removal of these useless tokens or to combine similar tokens? Add this into your code from part 1.\n",
    "\n",
    "By the end of exercise 3 you should have a data structure that stores information for all trials, and includes some form of cleaned tokens for use in training or building our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
