{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Homework #1 (for the updated JSON file)\n",
    "\n",
    "For anyone looking to try out their skills, try one or all of these exercises:\n",
    "\n",
    "Start with the data file: \"NEW_clinical_trial_data.json\" DO NOT USE the in-memory version of the data, as generated by the Jupyter Notebook \"Clinical Trial Data Extraction.ipynb\"\n",
    "\n",
    "The data structure used by clinicaltrials.ucsf.edu needs a number of fields and formatting choices that are important for keeping their webpage operational, but aren't ideal for our needs. However, if we want our modeling work to be useful, we need a way to easily join the outcome of our NLP model to the production JSON data structure.  The production data structure is a dictionary of dictionaries, with the primary key being the clinical trial \"nct_number\": an 11 digit unique identifier beginning with the string \"NCT\" (ie - 'NCT12345678').  Our NLP data structure, should therefore also be indexed by this identifier. \n",
    "\n",
    "Additionally, only certain trials are visible on the website, so we will need to know which trials are considered \"visible\" and \"joinable\", so we can appropriately categorize only the visible trials. The variable names in the JSON are 'is_joinable' and 'is_visible.'\n",
    "\n",
    "#### Exercise 1\n",
    "Your first task is to build a smaller test dataset that removes some of the fields we do not care about. Create a new data structure, and for each trial, copy over only the data that you believe would be useful as inputs to our model (plus the variables mentioned above) and save it to disk. Remember that modeling works best when we focus on the details that change appreciable between our unit of analysis (ie - trials.) In other words, your choise of variables to retain should not be things like \"sponsor\" or \"recruitment status\" because they won't really contribute much to our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('NEW_clinical_trial_data.json','r') as f:\n",
    "    data = json.load(f)\n",
    "keys = list(data.keys())\n",
    "mydata = {k:v for k,v in data.items() if v['is_visible'] and v['is_joinable']}\n",
    "mykeys = list(mydata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_include = ['nct_number','summary','summary_html','title_brief','title_official',\n",
    "              'keywords','conditions','description_html',\n",
    "              'eligibility_html','eligibility_inclusion_html','eligibility_exclusion_html',\n",
    "              'links','eligibility_summary_short', 'is_visible','is_joinable']\n",
    "# a dict of studies indexed by NCT number containing a dict of data fields\n",
    "filtered = {k:{i:j for i,j in v.items() if i in to_include} for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "Many of the most interesting fields (like 'summary') are riddled with formatting characters. This is great for hosting a website, but it is just noise for our NLP work.  For each of the free-text fields you've extracted in Exercise 1, remove all of the HTML formatting (Hint: The python package BeautifulSoup can be very useful here!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# start = 20\n",
    "# count = 10\n",
    "# for i in range(start,start+count):\n",
    "#     test_text = filtered[list(filtered.keys())[i]]['summary_html']\n",
    "#     if '&' not in test_text:\n",
    "#         i -=1\n",
    "#         continue\n",
    "#     print(test_text)\n",
    "#     sp = BeautifulSoup(test_text, 'html.parser')\n",
    "#     print('...')\n",
    "#     print(sp.get_text())\n",
    "#     print('\\n')\n",
    "\n",
    "# for study,attrs in filtered.items():\n",
    "#     for term in attrs:\n",
    "#         sp = BeautifulSoup(str(term), 'html.parser').get_text()\n",
    "#         if isinstance(v, (str,bytes)):\n",
    "#             filtered[k] = sp.get_text() \n",
    "#         else:\n",
    "#             print(v)\n",
    "#             input()\n",
    "\n",
    "# remove newlines, make all text lowercase\n",
    "filtered = {k:{label:BeautifulSoup(term, 'html.parser').get_text().replace('\\n',' ').lower() if isinstance(term, str) else term\n",
    "               for label,term in v.items()} \n",
    "            for k,v in filtered.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2a\n",
    "If you'd like to extend excercise 2, it would be useful to use Spacy/NLTK or something else to break the free text sections from excercise 2 into individual sentences and store them as elements in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4533 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|███████▉  | 3577/4533 [00:00<00:00, 34554.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 4533/4533 [00:00<00:00, 31691.77it/s]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# or nltk.sent_tokenize()\n",
    "\n",
    "# split long data fields into sentences\n",
    "filtered_sent_tok = {k:{label:term.split('.') if isinstance(term, str) else term\n",
    "               for label,term in v.items()} \n",
    "            for k,v in tqdm(filtered.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "Our goal for our modeling project is to categorize each of the trials into a unique category.  The language of clinicaltrials.ucsf.edu defines \"clusters\" as the primary category, and \"conditions\" as the sub-category.  Each trial can be classified as relating to multiple \"conditions\", and each condition can belong to multiple \"clusters.\" This complicates our modeling task appreciably. To simplify our project, we will assume that all \"conditions\" are adequately assigned (not a bad assumption, there are very few conditions = \"other\") and we will assume that any clincical trial that has at least 1 \"cluster\" not equal to \"Other\" can be categorized into that non-other cluster.  In other words, we are only interested in classifying trials that cannot be otherwise categorized.\n",
    "\n",
    "For exercise 3, please create a field in our data structure that indicates whether a specific trial has ONLY \"other\" clusters (even if there are more than 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4533 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 4533/4533 [00:00<00:00, 82540.09it/s]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# traverse the structure collecting all clusters, conditions, and cluster/condition pairs\n",
    "# at the same time, add a field 'is_other' that labels studies belonging exclusively to the 'Other' cluster\n",
    "\n",
    "conditions = set()\n",
    "clusters = set()\n",
    "pairs = set()\n",
    "slugs_to_names = dict()\n",
    "\n",
    "for k,v in tqdm(filtered.items()):\n",
    "    l_cond = v['conditions'] # a list of dicts\n",
    "    for cond in l_cond:\n",
    "        slugs_to_names[cond['slug']] = cond['name']\n",
    "        conditions |= set([cond['slug']])\n",
    "        clusters |= set(cond['clusters']) # a list\n",
    "        pairs |= set('{}:{}'.format(cond['slug'], clust) for clust in cond['clusters'])\n",
    "        if cond['clusters'] == ['Other']:\n",
    "            v['is_other'] = True\n",
    "    if 'is_other' not in v:\n",
    "        v['is_other'] = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the relationships of the clusters/conditions. More complete code in separate script, under Condition_deps_graph/.\n",
    "\n",
    "import graphviz\n",
    "\n",
    "dot = graphviz.Graph(format='svg', engine='neato', \n",
    "                     graph_attr={'overlap':'true',\n",
    "                                 'splines':'line',\n",
    "#                                  'Damping':'0.5',\n",
    "#                                  'overlap_shrink':'True',\n",
    "#                                  'pack':'True',\n",
    "                                 'quadtree':'2',\n",
    "                                 'minlen':'5'\n",
    "                                },\n",
    "                    node_attr={'sep':'0.055','margin':'0.055'})\n",
    "\n",
    "nodes_added = set()\n",
    "\n",
    "for j in pairs:\n",
    "    if 'Other' not in j:\n",
    "        one,two = j.split(':')\n",
    "        if one not in nodes_added:\n",
    "            dot.node(one)\n",
    "            nodes_added |= set(one)\n",
    "        if two not in nodes_added:\n",
    "            dot.node(two, fillcolor='lightskyblue', style='filled')\n",
    "            nodes_added |= set(two)\n",
    "        dot.edge(one, two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out.gv.svg'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.render('out.gv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
