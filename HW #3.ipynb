{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW #3 \n",
    "\n",
    "This month, we are focusing on transforming our text based data structure into a numeric one for use in one of several models (see the project overview flow chart). This numeric data structure is often referred to as a \"document term matrix\" and allows us to represent each document (ie - a trial, condition and cluster combination) as a vector. The collection of all of these documents gives us a matrix that we can use for modeling purposes.\n",
    "\n",
    "Encoding this text based data into vector form is done in a few different ways, but we will look into weighting techniques for building our numeric dataset. Typically this is done by representing each document by a single vector drawn from a vector space that spans the vocabularly defined within our document pool. Each of the components of this vector are some form of normalized or unnormalized word frequencies. How we define these frequencies is problem specific, but the most common scheme is referred to as TF-IDF. Here is some more information on this technique:\n",
    "\n",
    " - https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    " - http://ishwortimilsina.com/vector-space-model\n",
    "\n",
    "Once you've seen the project overview flow chart and read the above URLs, take a look at the Week 3 Vectorization Activity. Print it out if you can, this simple activity will give you a much stronger intuition for the types of vectorization schemes available.\n",
    "\n",
    "After the last homework, you should have a pretty well defined analytic data structure that links a specific condition and trial text with other features (if desired) to a cluster. It should be pretty easy to split those into \"other\" clusters and our \"training\" set of clusters. See the jupyter notebook called \"HW 2 Solutions\" for more information about this process.\n",
    "\n",
    "Next, we need to build our document-term matrix for use in modeling. The easiest way to do this is in Python to use the Sklearn TfidfVectorizer (as seen here https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af and here http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) on the full text (however you define that) for each document. This takes a step back from our tokenized words, but shows us the power of keeping all of that data around. For more info see: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "For this month's homework, please build the document term matrix for your training data. For this, each row should be 1 document (ie - a single Trial-condition combination) and each column will be one term from the corpus (ie - one of the words from the entire vocabulary used for all of our documents). Be sure to use one of sklearn's (or another python package) functions for doing this, as it will simplify your life immensely. \n",
    "\n",
    "Finally, there has been a slight update to the JSON data. I'm including it for reference as it is what I will be using, but you can continue to use the original version if that makes more sense for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiindex rows with the condition/cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW #3 Extension\n",
    "\n",
    "To extend this homework, we need to think about how we want to represent the conditions. To me, it seems like the conditions are very important features. We probably do not just want to use them as additional text. We may want to build indicator variables using them, but their dimensionality is huge, and that will probably hurt our model. How might we use the conditions in our model? My guess is that one of the NLM ontologies will be a big help here: MeSH or UMLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. clean with ontology tagging/thesauraus\n",
    "2. condense/hash into a feature (?)\n",
    "    - use as a weighted term, add to tf-idf?\n",
    "    - make indicators\n",
    "    - make a single axis categorical var\n",
    "    - ...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
